{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de8cedf-f817-423d-b53c-1f4ec4d1454b",
   "metadata": {},
   "source": [
    "# Stochastic Optimization\n",
    "\n",
    "## Exercise on Control\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218173a-8c0c-48f2-a36c-d13ad5de6461",
   "metadata": {},
   "source": [
    "Today you'll explore advanced SGD methods (momentum, Adam) and compare them with gradient-free approaches on a robotic arm control problem. You'll also learn automatic differentiation with JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760934ed-03db-41b4-b510-9241851ae253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from typing import Tuple, List, Callable\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable JAX to run on CPU (for consistent behavior across platforms)\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "print(\"Setup complete! JAX version:\", jax.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696a118-acec-4929-9eec-22187022c7f2",
   "metadata": {},
   "source": [
    "# Exploring the Robot Arm\n",
    "Run the provided visualization to understand the 2-joint planar arm. Experiment with different joint angles. We will experiment with two different libraries for this robot arm: numpy and [jax](docs.jax.dev/en/latest/index.html). Jax will enable easier gradient calculations through [automatic differentiation](https://huggingface.co/blog/andmholm/what-is-automatic-differentiation). For now, understand and explore this 2-joint robot arm which uses two angles to calculate the end position of the arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c89ae0-873d-4456-8e5c-9c2ad6d81363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_kinematics_numpy(theta: np.ndarray, lengths: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute end-effector position for a 2-joint planar arm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta : np.ndarray, shape (2,)\n",
    "        Joint angles [theta1, theta2] in radians\n",
    "    lengths : np.ndarray, shape (2,)\n",
    "        Link lengths [L1, L2]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    position : np.ndarray, shape (2,)\n",
    "        End-effector position [x, y]\n",
    "    \"\"\"\n",
    "    theta1, theta2 = theta\n",
    "    L1, L2 = lengths\n",
    "    \n",
    "    # First joint position\n",
    "    x1 = L1 * np.cos(theta1)\n",
    "    y1 = L1 * np.sin(theta1)\n",
    "    \n",
    "    # End-effector position\n",
    "    x2 = x1 + L2 * np.cos(theta1 + theta2)\n",
    "    y2 = y1 + L2 * np.sin(theta1 + theta2)\n",
    "    \n",
    "    return np.array([x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fefab2-358c-484c-a099-9539a5007e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX version (for automatic differentiation)\n",
    "def forward_kinematics_jax(theta: jnp.ndarray, lengths: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute end-effector position for a 2-joint planar arm (JAX version).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta : jnp.ndarray, shape (2,)\n",
    "        Joint angles [theta1, theta2] in radians\n",
    "    lengths : jnp.ndarray, shape (2,)\n",
    "        Link lengths [L1, L2]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    position : jnp.ndarray, shape (2,)\n",
    "        End-effector position [x, y]\n",
    "    \"\"\"\n",
    "    theta1, theta2 = theta\n",
    "    L1, L2 = lengths\n",
    "    \n",
    "    # First joint position\n",
    "    x1 = L1 * jnp.cos(theta1)\n",
    "    y1 = L1 * jnp.sin(theta1)\n",
    "    \n",
    "    # End-effector position\n",
    "    x2 = x1 + L2 * jnp.cos(theta1 + theta2)\n",
    "    y2 = y1 + L2 * jnp.sin(theta1 + theta2)\n",
    "    \n",
    "    return jnp.array([x2, y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430bcb2-173a-4c4d-9a8e-faa9070f92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all joint positions for visualization\n",
    "def get_arm_points(theta: np.ndarray, lengths: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get positions of all joints for visualization.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    points : np.ndarray, shape (3, 2)\n",
    "        Positions of [base, joint1, end-effector]\n",
    "    \"\"\"\n",
    "    theta1, theta2 = theta\n",
    "    L1, L2 = lengths\n",
    "    \n",
    "    # Base at origin\n",
    "    p0 = np.array([0.0, 0.0])\n",
    "    \n",
    "    # First joint\n",
    "    p1 = np.array([L1 * np.cos(theta1), L1 * np.sin(theta1)])\n",
    "    \n",
    "    # End-effector\n",
    "    p2 = p1 + np.array([L2 * np.cos(theta1 + theta2), \n",
    "                        L2 * np.sin(theta1 + theta2)])\n",
    "    \n",
    "    return np.array([p0, p1, p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219080b-95e7-4584-888e-b5b202b83a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementations\n",
    "print(\"Testing forward kinematics...\")\n",
    "test_theta = np.array([np.pi/4, np.pi/4])\n",
    "test_lengths = np.array([1.0, 1.0])\n",
    "\n",
    "pos_numpy = forward_kinematics_numpy(test_theta, test_lengths)\n",
    "pos_jax = forward_kinematics_jax(test_theta, test_lengths)\n",
    "\n",
    "print(f\"NumPy result: {pos_numpy}\")\n",
    "print(f\"JAX result: {pos_jax}\")\n",
    "print(f\"Match: {np.allclose(pos_numpy, np.array(pos_jax))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c8607-9dfa-47af-b464-1acd5982acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Utilities\n",
    "def plot_arm(theta: np.ndarray, lengths: np.ndarray, \n",
    "             target: np.ndarray = None, obstacles: List = None,\n",
    "             ax=None, title: str = \"Robot Arm\"):\n",
    "    \"\"\"\n",
    "    Plot the robot arm configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta : np.ndarray\n",
    "        Joint angles\n",
    "    lengths : np.ndarray\n",
    "        Link lengths\n",
    "    target : np.ndarray, optional\n",
    "        Target position to visualize\n",
    "    obstacles : List of tuples, optional\n",
    "        List of (x, y, radius) for circular obstacles\n",
    "    ax : matplotlib axis, optional\n",
    "        Axis to plot on\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "    # Get arm points\n",
    "    points = get_arm_points(theta, lengths)\n",
    "    \n",
    "    # Plot arm links\n",
    "    ax.plot(points[:, 0], points[:, 1], 'o-', linewidth=3, \n",
    "            markersize=8, label='Arm', color='blue')\n",
    "    \n",
    "    # Plot base\n",
    "    ax.plot(0, 0, 'ks', markersize=12, label='Base')\n",
    "    \n",
    "    # Plot end-effector\n",
    "    ax.plot(points[-1, 0], points[-1, 1], 'ro', \n",
    "            markersize=10, label='End-effector')\n",
    "    \n",
    "    # Plot target if provided\n",
    "    if target is not None:\n",
    "        ax.plot(target[0], target[1], 'g*', \n",
    "                markersize=15, label='Target')\n",
    "    \n",
    "    # Plot obstacles if provided\n",
    "    if obstacles is not None:\n",
    "        for obs in obstacles:\n",
    "            circle = Circle((obs[0], obs[1]), obs[2], \n",
    "                          color='red', alpha=0.3, label='Obstacle')\n",
    "            ax.add_patch(circle)\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e56ee-db07-41ad-a568-6dcdffb0d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard link lengths we'll use throughout\n",
    "LENGTHS = np.array([1.0, 1.0])\n",
    "\n",
    "# Example: Visualize different arm configurations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# TODO: try more configurations\n",
    "example_configs = [\n",
    "    (np.array([0.0, 0.0]), \"Fully Extended\"),\n",
    "    (np.array([np.pi/2, 0.0]), \"Elbow Straight Up\"),\n",
    "    (np.array([np.pi/4, np.pi/4]), \"45° - 45°\"),\n",
    "]\n",
    "\n",
    "target = np.array([1.1, 1.1])\n",
    "for i, (theta, title) in enumerate(example_configs):\n",
    "    plot_arm(theta, LENGTHS, target=target, ax=axes[i], title=title)\n",
    "    end_eff = forward_kinematics_numpy(theta, LENGTHS)\n",
    "    axes[i].text(0.05, 0.95, f'End-eff: ({end_eff[0]:.2f}, {end_eff[1]:.2f})',\n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22431f4b-a364-4075-971b-a8876ca70030",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q1: The robot arm has multiple solutions to reach the same point (elbow-up vs elbow-down configurations). Using the provided forward_kinematics function, find two different joint angle configurations θ = [θ₁, θ₂] that place the end-effector at approximately (1.1, 1.1). What does this tell you about the optimization landscape?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e63d3e-d44c-4ff7-be5f-65ad76a7c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1 solution and answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb8550-af8a-44a9-b59f-eec5c21316d1",
   "metadata": {},
   "source": [
    "## Part 1: Automatic Differentiation\n",
    "\n",
    "Instead of guessing at the angles for a target, we will calculate them exactly. Use the provided JAX implementation to compute gradients of the loss function with respect to joint angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f098564-027a-42ff-881f-fbe40782889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n",
    "def loss_target_reaching(theta: jnp.ndarray, lengths: jnp.ndarray, \n",
    "                        target: jnp.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Loss for reaching a target position.\n",
    "    \n",
    "    L = ||end_effector(theta) - target||^2\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta : jnp.ndarray, shape (2,)\n",
    "        Joint angles\n",
    "    lengths : jnp.ndarray, shape (2,)\n",
    "        Link lengths\n",
    "    target : jnp.ndarray, shape (2,)\n",
    "        Target position\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Squared distance to target\n",
    "    \"\"\"\n",
    "    end_effector = forward_kinematics_jax(theta, lengths)\n",
    "    return jnp.sum((end_effector - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a99f8-f645-4a3b-8176-1459f81dfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JIT-compiled versions for speed\n",
    "loss_target_reaching_jit = jit(loss_target_reaching)\n",
    "print(loss_target_reaching)\n",
    "print(loss_target_reaching_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58862ffa-2bdb-4158-aff9-17dd40c660ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gradient functions\n",
    "grad_loss_target = jit(grad(loss_target_reaching, argnums=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f015636-40f8-430c-bc64-8abba3b0f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loss functions\n",
    "print(\"Testing loss functions...\")\n",
    "test_theta = jnp.array([0.5, 0.5])\n",
    "test_lengths = jnp.array([1.0, 1.0])\n",
    "test_target = jnp.array([1.0, 1.0])\n",
    "\n",
    "loss_val = loss_target_reaching(test_theta, test_lengths, test_target)\n",
    "print(f\"Loss value: {loss_val:.4f}\")\n",
    "\n",
    "grad_val = grad_loss_target(test_theta, test_lengths, test_target)\n",
    "print(f\"Gradient: {grad_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2a0ec-7f97-43d5-95f8-b76f834fc13d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q2: Compute the gradient using JAX autodiff at $θ=[π/6, π/4]$\n",
    "Then verify it matches a manual finite-difference approximation:\n",
    "\n",
    "For finite differences, use:\n",
    "  $∂L/∂θᵢ ≈ (L(θ + h*eᵢ) - L(θ - h*eᵢ)) / (2h)$\n",
    "  where $eᵢ$ is the i-th unit vector and $h = 1e-5$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88da9fc-f166-4eed-bd60-0ec66ad52e34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q3: Compare the computational time of evaluating the gradient using JAX autodiff versus evaluating just the forward pass (loss calculation only). Run each 1000 times and report the ratio. What does this tell you about the computational overhead of automatic differentiation?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99be32-1927-4e98-a61b-2af42210011f",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent\n",
    "\n",
    "Gradient Descent (GD) is the foundational optimization algorithm that iteratively moves parameters in the direction of steepest descent:\n",
    "$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$\n",
    "\n",
    "While conceptually simple, GD can be sensitive to the learning rate $η$ and may struggle with saddle points or narrow valleys in the loss landscape where the gradient direction changes rapidly.\n",
    "\n",
    "For this exercise, implement basic gradient descent to move the robot arm to a target position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08b645-483c-4492-9cc4-4159d053d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(theta_init: np.ndarray, \n",
    "                     target: np.ndarray,\n",
    "                     lengths: np.ndarray,\n",
    "                     learning_rate: float = 0.01,\n",
    "                     n_iterations: int = 500,\n",
    "                     lambda_obs: float = 10.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Convert to JAX arrays\n",
    "    theta = jnp.array(theta_init)\n",
    "    lengths_jax = jnp.array(lengths)\n",
    "    target_jax = jnp.array(target)\n",
    "    \n",
    "\n",
    "    loss_fn = lambda th: loss_target_reaching(th, lengths_jax, target_jax)\n",
    "    grad_fn = grad_loss_target\n",
    "\n",
    "    # Storage for history\n",
    "    theta_history = np.zeros((n_iterations + 1, 2))\n",
    "    loss_history = np.zeros(n_iterations + 1)\n",
    "    ee_trajectory = np.zeros((n_iterations + 1, 2))\n",
    "    \n",
    "    # Initial values\n",
    "    theta_history[0] = np.array(theta)\n",
    "    loss_history[0] = float(loss_fn(theta))\n",
    "    ee_trajectory[0] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(n_iterations):\n",
    "        # TODO: Compute gradient\n",
    "        # gradient = ?\n",
    "        \n",
    "        # TODO: Update theta using gradient descent rule\n",
    "        # theta = ?\n",
    "        \n",
    "        # Store history\n",
    "        theta_history[i + 1] = np.array(theta)        \n",
    "        loss_history[i + 1] = float(loss_fn(theta))        \n",
    "        ee_trajectory[i + 1] = np.array(forward_kinematics_jax(theta, lengths_jax))\n",
    "    \n",
    "    return theta_history, loss_history, ee_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e55d0-6b19-4547-8c8a-d84c5d2e1181",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q4: Run GD with learning rates $η = [0.001, 0.01, 0.1, 0.5, 1.0]$ for 500 iterations each, starting from $θ₀ = [0.1, 0.1]$ targeting (0.5, 1.5). Plot the loss curves. At what learning rate do you observe divergence? Explain what's happening in terms of the gradient descent update rule.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047ff70-465b-44c1-bc55-ca15c349190f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q5: For $η = 0.01$, plot the end-effector trajectory in 2D space (not the loss, but the actual path the end-effector takes). Does it take the most direct path to the target? Why or why not? (Hint: think about parameter space vs task space)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdcd034-bd9f-4ae4-a8db-a6eb9f1951d6",
   "metadata": {},
   "source": [
    "## 2.2: SGD with Momentum\n",
    "\n",
    "Momentum adds \"inertia\" to gradient descent by accumulating a velocity vector that combines the current gradient with previous gradients: $_t = \\beta v_{t-1} + \\nabla L(\\theta_t)$\n",
    ", then $\\theta_{t+1} = \\theta_t - \\eta v_t$. This helps the optimizer build up speed in consistent directions and dampens oscillations in directions where gradients frequently change sign, allowing it to better navigate ravines and escape shallow local minima.\n",
    "\n",
    "\n",
    "For this exercise, implement momentum-based gradient descent using the update rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c380f1-40f1-4322-821d-3c87dfc169ab",
   "metadata": {},
   "source": [
    "$v_t = βv_{t-1} + ∇L(θ_t)$\n",
    "\n",
    "$θ_{t+1} = θ_t - ηv_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c05bc-8df3-47f4-86c2-4917819e49e7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q6: Compare GD vs Momentum ($β=0.9$) on the same task ($η=0.01$, target (0.5, 1.5)). Plot both the loss curves and the end-effector trajectories side-by-side. Describe one specific, concrete difference in how the arm moves with momentum versus without.\n",
    "</div>\n",
    "<div class=\"alert alert-info\">\n",
    "Q7: Track and plot the magnitude of the velocity vector $||v_t||$ over iterations for the momentum optimizer. What happens to this magnitude as the arm approaches the target? Explain why this behavior occurs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a0bbc-d1f7-4a48-a3f2-31a494da937e",
   "metadata": {},
   "source": [
    "## 2.3: Adam Optimizer\n",
    "\n",
    "Adam (Adaptive Moment Estimation) combines ideas from momentum and adaptive learning rates by maintaining both a moving average of gradients (first moment $m_t$) and a moving average of squared gradients (second moment $v_t$).\n",
    "\n",
    "By dividing the update by $\\sqrt{v_t}$ Adam automatically adjusts the effective learning rate for each parameter based on the history of gradient magnitudes—parameters with large, consistent gradients get smaller effective steps, while parameters with small or noisy gradients get larger effective steps, making it particularly robust across different types of loss landscapes.\n",
    "\n",
    "For this exercise, extend your Momentum SGD to include the second moment, following this update:\n",
    "\n",
    "Update rules:\n",
    "$\n",
    "\\begin{align}\n",
    "g_t &= \\nabla L(\\theta_t) \\\\\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f693a0-f27c-4b81-8ed8-6b19906f4021",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q8: Start the arm at $θ = [2.5, -2.0]$ trying to reach target ($0.8, 0.8$). Compare how many iterations GD ($η=0.01$), Momentum ($β=0.9, η=0.01$), and Adam (default parameters) each need to get within distance $0.01$ of the target. Report the iteration counts and explain the ranking.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be9343-5fd0-4359-a3ee-172330f7e319",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q9: For the Adam optimizer on the same task, plot the effective per-parameter learning rate ($η_eff = η * m̂_t / (√v̂_t + ε)$) for both $θ₁$ and $θ₂$ over time. Do they receive the same effective learning rate throughout optimization? Explain why this adaptive behavior is useful.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556fcfa-fcf1-4b81-9018-cd87858fcfd8",
   "metadata": {},
   "source": [
    "## Part 3: Gradient-Free Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cd6a9-fe47-4f3a-963b-5d7aedef73bb",
   "metadata": {},
   "source": [
    "For this exercise, implement a basic gradient-free algorithm, specifically an evolutionary strategy. Evolution Strategies are a class of gradient-free optimization algorithms inspired by natural evolution. Unlike gradient-based methods that require computing derivatives, ES samples multiple candidate solutions (a \"population\") around the current solution, evaluates their fitness (objective function value), and moves toward the direction indicated by the better-performing samples. Use the following rules for your ES:\n",
    "\n",
    "For iteration $t$, with current parameter vector $\\theta_t$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\epsilon_i &\\sim \\mathcal{N}(0, I) \\quad \\text{for } i = 1, \\ldots, N \\\\\n",
    "\\theta_i &= \\theta_t + \\sigma \\epsilon_i \\\\\n",
    "F_i &= -L(\\theta_i) \\quad \\text{(fitness = negative loss)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t + \\alpha \\frac{1}{N} \\sum_{i=1}^{N} F_i \\epsilon_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $N$ is the population size, $\\sigma$ is the exploration noise (standard deviation), $\\alpha$ is the learning rate, and $\\epsilon_i$ are the perturbation vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8793600-ca32-4b68-8bd6-b22633eca687",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Q10: Run this ES with population sizes $P = [10, 20, 50]$. For each, report: (a) does it reach the target successfully? (b) how many iterations does it take? (c) total number of function evaluations ($P$ × iterations). What trend do you observe with increasing population size?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ad7b9-35c3-484f-b64b-52539ea4541c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q11: ES doesn't use gradients, so it evaluates the loss N times per iteration while SGD evaluates it once plus one gradient computation. For a successful run of each method, count total loss function evaluations. Which is more sample-efficient? Despite this, describe one scenario where ES might still be preferred over gradient-based methods.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ce4ad-739b-450e-b18f-4a63500dde1e",
   "metadata": {},
   "source": [
    "## Part 4: Trajectory Optimization - Optimizing Sequences\n",
    "\n",
    "So far, we've only optimized the **final configuration** of the robot arm - finding joint angles that place the end-effector at the target. However, in real robotics, we often care about the **entire motion path**. A robot might reach the target, but if it does so with jerky, energy-intensive movements, the solution isn't practical.\n",
    "\n",
    "In this section, you'll optimize a **trajectory** - a sequence of joint angles over time - to reach the target while minimizing energy consumption and ensuring smooth motion.\n",
    "\n",
    "### Trajectory Representation\n",
    "\n",
    "Instead of optimizing $\\theta \\in \\mathbb{R}^2$, we now optimize:\n",
    "$$\\Theta = [\\theta_0, \\theta_1, \\ldots, \\theta_T] \\in \\mathbb{R}^{T \\times 2}$$\n",
    "\n",
    "where $T$ is the number of timesteps (e.g., $T=20$).\n",
    "\n",
    "### Multi-Objective Loss Function\n",
    "\n",
    "We balance three competing objectives:\n",
    "\n",
    "$$L_{total}(\\Theta) = L_{target} + \\lambda_{energy} L_{energy} + \\lambda_{smooth} L_{smooth}$$\n",
    "\n",
    "where:\n",
    "- **Target reaching**: $L_{target} = \\|\\text{pos}(\\theta_T) - \\text{target}\\|^2$ (only final position matters)\n",
    "- **Energy cost**: $L_{energy} = \\sum_{t=1}^{T} \\|\\theta_t - \\theta_{t-1}\\|^2$ (penalize large joint movements)\n",
    "- **Smoothness**: $L_{smooth} = \\sum_{t=2}^{T} \\|(\\theta_t - \\theta_{t-1}) - (\\theta_{t-1} - \\theta_{t-2})\\|^2$ (penalize acceleration/jerkiness)\n",
    "\n",
    "The hyperparameters $\\lambda_{energy}$ and $\\lambda_{smooth}$ control the trade-off between reaching the target quickly versus moving efficiently and smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a96b9-7a07-467b-a891-aad0c48dfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Optimization Functions\n",
    "\n",
    "def loss_trajectory(theta_sequence: jnp.ndarray, \n",
    "                   lengths: jnp.ndarray,\n",
    "                   target: jnp.ndarray,\n",
    "                   lambda_energy: float = 0.1,\n",
    "                   lambda_smooth: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Loss for trajectory optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta_sequence : jnp.ndarray, shape (T, 2)\n",
    "        Sequence of joint angles over T timesteps\n",
    "    lengths : jnp.ndarray, shape (2,)\n",
    "        Link lengths\n",
    "    target : jnp.ndarray, shape (2,)\n",
    "        Target position\n",
    "    lambda_energy : float\n",
    "        Weight for energy cost\n",
    "    lambda_smooth : float\n",
    "        Weight for smoothness cost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    loss : float\n",
    "        Total loss combining target reaching, energy, and smoothness\n",
    "    \"\"\"\n",
    "    T = theta_sequence.shape[0]\n",
    "    \n",
    "    # Target reaching: only final position matters\n",
    "    final_pos = forward_kinematics_jax(theta_sequence[-1], lengths)\n",
    "    loss_target = jnp.sum((final_pos - target) ** 2)\n",
    "    \n",
    "    # Energy cost: sum of squared velocities (joint movements)\n",
    "    velocities = theta_sequence[1:] - theta_sequence[:-1]  # Shape: (T-1, 2)\n",
    "    loss_energy = jnp.sum(velocities ** 2)\n",
    "    \n",
    "    # Smoothness cost: sum of squared accelerations\n",
    "    accelerations = velocities[1:] - velocities[:-1]  # Shape: (T-2, 2)\n",
    "    loss_smooth = jnp.sum(accelerations ** 2)\n",
    "    \n",
    "    return loss_target + lambda_energy * loss_energy + lambda_smooth * loss_smooth\n",
    "\n",
    "\n",
    "# Create gradient function for trajectory optimization\n",
    "grad_loss_trajectory = jit(grad(loss_trajectory, argnums=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72287d-557a-4e7e-973d-0c170b0a834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_sequence(theta_sequence: np.ndarray,\n",
    "                             lengths: np.ndarray,\n",
    "                             target: np.ndarray = None,\n",
    "                             title: str = \"Arm Trajectory Over Time\"):\n",
    "    \"\"\"\n",
    "    Visualize the robot arm at multiple points along the trajectory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta_sequence : np.ndarray, shape (T, 2)\n",
    "        Sequence of joint angles\n",
    "    lengths : np.ndarray\n",
    "        Link lengths\n",
    "    target : np.ndarray, optional\n",
    "        Target position\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    T = len(theta_sequence)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, T))\n",
    "    \n",
    "    # Plot each arm configuration\n",
    "    for t in range(0, T, max(1, T//10)):  # Show ~10 frames\n",
    "        points = get_arm_points(theta_sequence[t], lengths)\n",
    "        alpha = 0.3 + 0.7 * (t / T)  # Fade from transparent to opaque\n",
    "        ax.plot(points[:, 0], points[:, 1], 'o-', \n",
    "               color=colors[t], alpha=alpha, linewidth=2, markersize=4)\n",
    "    \n",
    "    # Plot final configuration more prominently\n",
    "    points_final = get_arm_points(theta_sequence[-1], lengths)\n",
    "    ax.plot(points_final[:, 0], points_final[:, 1], 'o-', \n",
    "           color='red', linewidth=3, markersize=8, label='Final')\n",
    "    \n",
    "    # Plot end-effector trajectory\n",
    "    ee_traj = np.array([forward_kinematics_numpy(theta_sequence[t], lengths) \n",
    "                        for t in range(T)])\n",
    "    ax.plot(ee_traj[:, 0], ee_traj[:, 1], 'b--', \n",
    "           linewidth=2, alpha=0.5, label='End-effector path')\n",
    "    \n",
    "    # Plot target\n",
    "    if target is not None:\n",
    "        ax.plot(target[0], target[1], 'g*', markersize=20, label='Target')\n",
    "    \n",
    "    # Plot base\n",
    "    ax.plot(0, 0, 'ks', markersize=12, label='Base')\n",
    "    \n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec71a92-d796-4b8f-a943-87043fa024fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_trajectory_metrics(theta_sequence: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute and print trajectory quality metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta_sequence : np.ndarray, shape (T, 2)\n",
    "        Sequence of joint angles\n",
    "    \"\"\"\n",
    "    velocities = np.diff(theta_sequence, axis=0)\n",
    "    accelerations = np.diff(velocities, axis=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    total_movement = np.sum(np.abs(velocities))\n",
    "    max_velocity = np.max(np.linalg.norm(velocities, axis=1))\n",
    "    max_acceleration = np.max(np.linalg.norm(accelerations, axis=1))\n",
    "    smoothness = np.sum(accelerations ** 2)  # Lower is smoother\n",
    "    \n",
    "    print(\"Trajectory Metrics:\")\n",
    "    print(f\"  Total joint movement: {total_movement:.4f} radians\")\n",
    "    print(f\"  Max velocity: {max_velocity:.4f} rad/timestep\")\n",
    "    print(f\"  Max acceleration: {max_acceleration:.4f} rad/timestep²\")\n",
    "    print(f\"  Smoothness cost: {smoothness:.4f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687edf6-fb7a-4eec-9e5e-c6dab976ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trajectory optimization with gradient descent\n",
    "def gradient_descent_trajectory(theta_init: np.ndarray,\n",
    "                               target: np.ndarray,\n",
    "                               lengths: np.ndarray,\n",
    "                               T: int = 20,\n",
    "                               learning_rate: float = 0.01,\n",
    "                               n_iterations: int = 1000,\n",
    "                               lambda_energy: float = 0.1,\n",
    "                               lambda_smooth: float = 0.05):\n",
    "    # Initialize trajectory: linear interpolation from init to a guess\n",
    "    target_angles = theta_init + 0.5  # Simple guess for final angles\n",
    "    theta_sequence = jnp.array([\n",
    "        theta_init + (target_angles - theta_init) * t / (T - 1)\n",
    "        for t in range(T)\n",
    "    ])\n",
    "    \n",
    "    lengths_jax = jnp.array(lengths)\n",
    "    target_jax = jnp.array(target)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Compute loss and gradient\n",
    "        loss_val = loss_trajectory(theta_sequence, lengths_jax, target_jax,\n",
    "                                   lambda_energy, lambda_smooth)\n",
    "        gradient = grad_loss_trajectory(theta_sequence, lengths_jax, target_jax,\n",
    "                                       lambda_energy, lambda_smooth)\n",
    "        \n",
    "        # Update trajectory\n",
    "        theta_sequence = theta_sequence - learning_rate * gradient\n",
    "        \n",
    "        loss_history.append(float(loss_val))\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}: Loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return np.array(theta_sequence), np.array(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738ae3f-d241-4821-87de-e8296d6c59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Trajectory Optimization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "theta_init = np.array([0.1, 0.1])\n",
    "target = np.array([1.0, 1.5])\n",
    "T = 20\n",
    "\n",
    "print(f\"\\nOptimizing trajectory with T={T} timesteps\")\n",
    "print(f\"Initial configuration: {theta_init}\")\n",
    "print(f\"Target position: {target}\\n\")\n",
    "\n",
    "theta_traj, loss_hist = gradient_descent_trajectory(\n",
    "    theta_init, target, LENGTHS,\n",
    "    T=T, learning_rate=0.01, n_iterations=1000,\n",
    "    lambda_energy=0.1, lambda_smooth=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d192c3-d6b5-4091-97e5-865a9a378b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "plot_loss_curve(loss_hist, \"Trajectory Optimization Loss\")\n",
    "plt.show()\n",
    "\n",
    "plot_trajectory_sequence(theta_traj, LENGTHS, target)\n",
    "plt.show()\n",
    "\n",
    "analyze_trajectory_metrics(theta_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80097d9-b148-48cd-af42-aa16e8a66f93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q12: Run trajectory optimization with three different energy weights: $\\lambda_{energy} = [0.01, 0.1, 1.0]$ (keep $\\lambda_{smooth}=0.05$ fixed). For each, visualize the resulting trajectory and report the total joint movement (sum of velocity magnitudes). How does increasing the energy penalty affect the motion? Does the arm still reach the target?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13087ba-382a-4bbc-949c-c9826dab6e65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q13: Compare trajectory optimization using SGD with Momentum vs Adam. Use $T=20$, $\\lambda_{energy}=0.1$, $\\lambda_{smooth}=0.05$. Which converges faster? Plot both trajectories side-by-side. Do they find qualitatively different solutions (different motion strategies)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c352f75-0ce8-42da-8b83-7f995d8836c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q14: The trajectory optimization problem has $T \\times 2$ parameters (40 parameters for $T=20$). This is similar to optimizing a small neural network. Based on your experiments, which optimizer (GD, Momentum, Adam) would you recommend for optimizing trajectories? How might your answer change if $T=100$ (200 parameters)?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6544a73-506c-4401-834b-8a487ddd4002",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Q15: Gradient-free methods can also optimize trajectories! However, with $T \\times 2$ parameters, the search space becomes much larger. Implement ES for trajectory optimization. Use a smaller population size initially (N=10) and $T=10$ timesteps to keep computation manageable. Compare sample efficiency (total function evaluations) with Adam. When might ES be preferred despite being less sample-efficient?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8d351-7432-4142-a28f-77c904502de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
